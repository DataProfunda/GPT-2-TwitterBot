{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 9097,
     "status": "ok",
     "timestamp": 1682287107319,
     "user": {
      "displayName": "konstanty marczak",
      "userId": "09925184578633414249"
     },
     "user_tz": -120
    },
    "id": "UCI2fg3pDUSz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "from auth_tw import get_key\n",
    "import tweepy\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(MODEL_EPOCH=4):\n",
    "    \"\"\"\n",
    "    Function to load a trained GPT-2 model from a specified epoch.\n",
    "\n",
    "    Args:\n",
    "    - MODEL_EPOCH (int): The epoch of the model to load (default is 4).\n",
    "\n",
    "    Returns:\n",
    "    - model (GPT2LMHeadModel): The loaded GPT-2 model.\n",
    "    - tokenizer (GPT2Tokenizer): The tokenizer corresponding to the loaded model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the folder where trained models are stored\n",
    "    models_folder = \"../trained_models\"\n",
    "\n",
    "    # Construct the path to the model file for the specified epoch\n",
    "    model_path = os.path.join(models_folder, f\"gpt2_xl_manbot_{MODEL_EPOCH}.pt\")\n",
    "\n",
    "    # Initialize the tokenizer with the base GPT-2 model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # Initialize the GPT-2 model for language modeling\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # Load the state dict of the model parameters from the specified path\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Move the model to the specified device (GPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Return the loaded model and tokenizer\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()\n",
    "\n",
    "data_twq = pd.read_csv('data_twq.csv',sep=';').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_first_word(tweet):\n",
    "    return str(tweet[0].split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a function 'return_first_word' to each row of the DataFrame 'data_twq'\n",
    "# The result is stored in the 'first_words' Series\n",
    "first_words = data_twq.apply(return_first_word, axis=1).copy()\n",
    "\n",
    "# Calculate the total count of occurrences of each first word in the 'first_words' Series\n",
    "sumInstances = pd.DataFrame(first_words).value_counts().sum()\n",
    "\n",
    "# Create a DataFrame containing the unique first words\n",
    "words = pd.DataFrame(pd.DataFrame(first_words).value_counts().index.tolist())\n",
    "\n",
    "# Calculate the probability of each first word occurrence\n",
    "propability = pd.DataFrame(pd.DataFrame(first_words).value_counts().values / sumInstances)\n",
    "\n",
    "# Join the 'words' DataFrame and the 'propability' DataFrame based on index\n",
    "word_prob = words.join(propability,how='left', lsuffix='_left')\n",
    "\n",
    "# Rename the columns of the resulting DataFrame\n",
    "word_prob.columns = ['word', 'prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    # Find the indices of the top n elements in probs\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "\n",
    "    # Get the top n probabilities\n",
    "    top_prob = probs[ind]\n",
    "\n",
    "    # Normalize the probabilities to make sure they sum up to 1\n",
    "    top_prob = top_prob / np.sum(top_prob) \n",
    "\n",
    "    # Randomly choose an index based on the normalized probabilities\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "\n",
    "    # Get the token_id corresponding to the chosen index\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_on_twitter(tweet_text = \"\"):\n",
    "    \"\"\"\n",
    "    Function to post a tweet on Twitter.\n",
    "\n",
    "    Args:\n",
    "    - tweet_text (str): The text content of the tweet (default is an empty string).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Authenticate with Twitter API using OAuth\n",
    "    auth = tweepy.OAuthHandler(get_key(\"api_key\"), get_key(\"api_key_secret\"))\n",
    "    auth.set_access_token(get_key(\"access_token\"), get_key(\"access_token_secret\"))\n",
    "\n",
    "    # Create a client using tweepy.Client\n",
    "    client = tweepy.Client(bearer_token=get_key(\"bearer_token\"), consumer_key=get_key(\"api_key\"), consumer_secret=get_key(\"api_key_secret\"), access_token=get_key(\"access_token\"), access_token_secret=get_key(\"access_token_secret\"))\n",
    "\n",
    "    try:\n",
    "        # Attempt to create a tweet with the provided text\n",
    "        response = client.create_tweet(text=tweet_text)\n",
    "        tweet_id =  response.data['id']\n",
    "        print('Tweet posted successfully! Tweet ID:', tweet_id)\n",
    "    except Exception as e:\n",
    "        print('Error occurred while posting the tweet:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(random = True,start_with='',output_file='generated_content.txt', size=5, post_on_twitter = False):\n",
    "    \"\"\"\n",
    "    Function to generate content (tweets) based on a trained language model.\n",
    "\n",
    "    Args:\n",
    "    - random (bool): Flag indicating whether to start with a random word (default is True).\n",
    "    - start_with (str): The starting word for generation if 'random' is set to False (default is an empty string).\n",
    "    - output_file (str): The file path to save the generated content (default is 'generated_content.txt').\n",
    "    - size (int): The number of tweets to generate (default is 5).\n",
    "    - post_on_twitter (bool): Flag indicating whether to post generated content on Twitter (default is False).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the file path to save the generated content\n",
    "    output_file_path = f'{output_file}'\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remove the existing file if it already exist\n",
    "    if os.path.exists(output_file_path):\n",
    "        os.remove(output_file_path)\n",
    "    \n",
    "    tweet_num = 0\n",
    "\n",
    "    # Perform generation without gradient computation\n",
    "    with torch.no_grad():\n",
    "   \n",
    "        for tweet_idx in range(size):\n",
    "        \n",
    "            tweet_finished = False\n",
    "            first_word = ''\n",
    "            \n",
    "            # Choose the first word randomly from the word probability distribution\n",
    "            if random: \n",
    "                first_word = word_prob['word'][np.random.choice(np.arange(len(word_prob)),p=word_prob['prob'])]\n",
    "            else:\n",
    "                first_word = start_with\n",
    "\n",
    "            # Encode the first word and convert it to a tensor\n",
    "            cur_ids = torch.tensor(tokenizer.encode(first_word)).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate tokens to complete the tweet\n",
    "            for i in range(100):\n",
    "                outputs = model(cur_ids, labels=cur_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "\n",
    "                # Adjust the top-N sampling based on the iteration\n",
    "                if i < 3:\n",
    "                    n = 20\n",
    "                else:\n",
    "                    n = 3\n",
    "\n",
    "                # Select the next token using top-N sampling\n",
    "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "                # Check if tweet generation is complete\n",
    "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                    tweet_finished = True\n",
    "                    break\n",
    "\n",
    "            # If tweet generation is complete, process and write to file\n",
    "            if tweet_finished:\n",
    "                \n",
    "                tweet_num = tweet_num + 1                \n",
    "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "                output_text = tokenizer.decode(output_list)\n",
    "\n",
    "                # Optionally post the generated content on Twitter\n",
    "                if post_on_twitter:\n",
    "                    post_on_twitter(output_text)\n",
    "                print(output_text)\n",
    "\n",
    "                # Write the generated tweet to the output file\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as f:\n",
    "                    f.write(f\"{output_text} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 92423,
     "status": "ok",
     "timestamp": 1682287467293,
     "user": {
      "displayName": "konstanty marczak",
      "userId": "09925184578633414249"
     },
     "user_tz": -120
    },
    "id": "RSeLQZBeDj_P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corporations profit from social media.\n",
      "\n",
      "The average person has no idea what they're doing.\n",
      "\n",
      "The average person doesn't care.\n",
      "\n",
      "The average person thinks they're the best thing they're doing.\n",
      "\n",
      "The average person doesn't care.<|endoftext|>\n",
      "Help them love what you are doing.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#Generate 1 tweet, save it to random5_generated.txt file and post a tweet containing generated text\n",
    "generate_content(output_file = 'random5_generated.txt', size=1,post_on_twitter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start tweets with \"Driving\" \n",
    "generate_content(False, 'Driving ',output_file = 'Driving_generated.txt', size=50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO9MmquDaRiTqqqY1/yf8vU",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
